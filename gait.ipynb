{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUCLfsWY2kih"
   },
   "source": [
    "## Sections:\n",
    "\n",
    "1.   Load and prepare data\n",
    "2.   Data preprocessing\n",
    "\n",
    "    *   Filter and noise reduction\n",
    "    *   Extract motion sequence\n",
    "    *   Rotate data\n",
    "    *   Segment trial into samples\n",
    "    *   Resampling and normalization\n",
    "    *   Remove malicious steps\n",
    "3.   Neural Network\n",
    "    \n",
    "    *   Data set-up\n",
    "    *   Training and evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH2GaCAusD8q"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F85Y2MT82kih"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from scipy.signal import butter, sosfilt\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2RDILucqOAW"
   },
   "source": [
    "# 1. Load and prepare data\n",
    "\n",
    "Define directory of Smartphone data.\n",
    "\n",
    "The folder should contain:\n",
    "* The subfolders of all experiments\n",
    "\n",
    "The parent folder should contain \n",
    "* anthro.xlsx\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Abd5adaEPCH4"
   },
   "outputs": [],
   "source": [
    "input_dir = 'C:/Users/Mridu/Desktop/CIiE/Project/test/'\n",
    "output_dir = '' \n",
    "input_anthropology_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbHqMNXIJfoU"
   },
   "outputs": [],
   "source": [
    "def get_frequency(data): \n",
    "    # gets the frequency of the accelerometer/gyroscope data and returns it as a dataframe\n",
    "    time = data.iloc[:]['Time [s]']\n",
    "    time = np.array(pd.DataFrame(time))     \n",
    "\n",
    "    time_duration = time.max() - time.min()\n",
    "    number_datapoints = time.size\n",
    "    \n",
    "    frequency = number_datapoints/time_duration\n",
    "    \n",
    "    frequency_df = pd.DataFrame(data = ([frequency]), columns = ['frequency [Hz]'])\n",
    "    \n",
    "    return frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdoTGAXAq_D_"
   },
   "outputs": [],
   "source": [
    "def unify_anthropometric_data(anthropometric_data = pd.DataFrame()):        \n",
    "    # trys to unify the manual written anthropology data\n",
    "    # i used the data as string to easyly change the commas to points and to cut out any possible letters (e.g.: kg)\n",
    "    \n",
    "    sex = anthropometric_data.iloc[0]['subject_sex']\n",
    "    sex_str = str(sex)\n",
    "    height = anthropometric_data.iloc[0]['subject_height [m]']\n",
    "    height_str = str(height)                                                \n",
    "    height_str = height_str.replace(',','.')\n",
    "    weight = anthropometric_data.iloc[0]['subject_weight [kg]']\n",
    "    weight_str = str(weight)\n",
    "    weight_str = weight_str.replace(',','.')\n",
    "    age = anthropometric_data.iloc[0]['subject_age [y]']\n",
    "    age_str = str(age)\n",
    "    \n",
    "    if ('f' in sex_str) or ('w' in sex_str) or ('F' in sex_str) or ('W' in sex_str):                 # checks for 'F/female' and 'W/woman'\n",
    "        \n",
    "        anthropometric_data.at[0,'subject_sex'] = 0     # 0 = female\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        anthropometric_data.at[0,'subject_sex'] = 1     # 1 = male\n",
    "        \n",
    "    for i in range(len(height_str)):\n",
    "        \n",
    "        if not (height_str[i].isdigit() or (height_str[i] == '.')):\n",
    "            \n",
    "            height_str = height_str[0:i]                # cuts of the string after the numbers end ('1.85 m' -> '1.85')\n",
    "            \n",
    "            break\n",
    "     \n",
    "    if not (len(height_str) == 0):    \n",
    "                \n",
    "        if float(height_str) > 100:\n",
    "            \n",
    "            height_str = float(height_str)/100              # makes sure the unit is [m] and not [cm]\n",
    "        \n",
    "    for i in range(len(weight_str)):\n",
    "        \n",
    "        if not (weight_str[i].isdigit() or (weight_str[i] == '.')):\n",
    "            \n",
    "            weight_str = weight_str[0:i]                # cuts of the string after the numbers end ('65 kg' -> '65')\n",
    "            \n",
    "            break    \n",
    "        \n",
    "    for i in range(len(age_str)):\n",
    "        \n",
    "        if not age_str[i].isdigit():\n",
    "            \n",
    "            age_str = age_str[0:i]                      # cuts of the string after the numbers end ('25 y' -> '25')\n",
    "            \n",
    "            break\n",
    "    \n",
    "    if height_str and weight_str and age_str:\n",
    "        \n",
    "        anthropometric_data.at[0,'subject_height [m]'] = float(height_str)  # updating the data\n",
    "        anthropometric_data.at[0,'subject_weight [kg]'] = float(weight_str) # updating the data\n",
    "        anthropometric_data.at[0,'subject_age [y]'] = float(age_str)        # updating the data\n",
    "        \n",
    "        return anthropometric_data\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQaY6ArPribW"
   },
   "outputs": [],
   "source": [
    "def unify_measurement_label(data_in = pd.DataFrame(), data_name = ''):\n",
    "    # trys to unify the different sensor names/labels of the different smartphones\n",
    "    \n",
    "    data_out = data_in\n",
    "    \n",
    "    for column in data_in.columns:\n",
    "        \n",
    "        if ('Time' in column) or ('time' in column):\n",
    "            \n",
    "            data_out = data_out.rename(columns = {column: 'Time [s]'})\n",
    "        \n",
    "        elif ('acc' in column) or ('Acc' in column) or ('m/s^2' in column):\n",
    "            \n",
    "            if ('x' in column) or ('X' in column):\n",
    "                \n",
    "                data_out = data_out.rename(columns = {column: 'acceleration_x [m/s^2]'})\n",
    "\n",
    "            elif ('y' in column) or ('Y' in column):\n",
    "                \n",
    "                data_out = data_out.rename(columns = {column: 'acceleration_y [m/s^2]'})\n",
    "                \n",
    "            elif ('z' in column) or ('Z' in column):\n",
    "                \n",
    "                data_out = data_out.rename(columns = {column: 'acceleration_z [m/s^2]'})\n",
    "                \n",
    "        elif ('gyroscope' in column) or ('Gyroscope' in column) or ('rad' in column):\n",
    "            \n",
    "            if ('x' in column) or ('X' in column):\n",
    "                \n",
    "                data_out = data_out.rename(columns = {column: 'angular_velocity_x [rad/s]'})\n",
    "\n",
    "            elif ((column.count('y') == 2) and ('gyroscope' in column)) or ((column.count('y') == 2) and ('Gyroscope' in column)) or (\n",
    "                    ('y' in column) and ('rad' in column) and (not(('gyroscope' in column) or ('Gyroscope' in column)))) or (\n",
    "                    ('Y' in column) and ('rad' in column) and (not(('gyroscope' in column) or ('Gyroscope' in column)))\n",
    "                    ) or ((column.count('Y') == 2) and ('gyroscope' in column)) or ((column.count('Y') == 2) and ('Gyroscope' in column)):\n",
    "                \n",
    "                data_out = data_out.rename(columns = {column: 'angular_velocity_y [rad/s]'})\n",
    "                \n",
    "            elif ('z' in column) or ('Z' in column):\n",
    "                \n",
    "                data_out = data_out.rename(columns = {column: 'angular_velocity_z [rad/s]'})\n",
    "            \n",
    "    if not (('Time [s]' in data_out.columns) and (('acceleration_x [m/s^2]' in data_out.columns) or ('angular_velocity_x [rad/s]' in data_out.columns)\n",
    "    ) and (('acceleration_y [m/s^2]' in data_out.columns) or ('angular_velocity_y [rad/s]' in data_out.columns)) and (\n",
    "             ('acceleration_z [m/s^2]' in data_out.columns) or ('angular_velocity_z [rad/s]' in data_out.columns)) and (data_out.columns.size == 4)):\n",
    "        \n",
    "        if not data_name:\n",
    "            \n",
    "            return pd.DataFrame()\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            if 'Acc' in  data_name:\n",
    "                \n",
    "                if data_in.columns.size == 5:\n",
    "        \n",
    "                    data_out = pd.DataFrame(data = np.array(data_in)[:,1:], columns = ['Time [s]', 'acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]'])\n",
    "                    \n",
    "                else:\n",
    "                \n",
    "                    data_out = pd.DataFrame(data = np.array(data_in), columns = ['Time [s]', 'acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]'])\n",
    "                \n",
    "                \n",
    "                \n",
    "            elif 'Gyr' in data_name:\n",
    "                \n",
    "                if data_in.columns.size == 5:\n",
    "        \n",
    "                    data_out = pd.DataFrame(data = np.array(data_in)[:,1:], columns = ['Time [s]', 'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]'])\n",
    "                \n",
    "                else:\n",
    "                \n",
    "                    data_out = pd.DataFrame(data = np.array(data_in), columns = ['Time [s]', 'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]'])\n",
    "        \n",
    "        \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4vcNs-xHXTG"
   },
   "outputs": [],
   "source": [
    "def initialize_print_progress(data_folders, notify_progress_threshold = 10):\n",
    "    # initializes the parameters needed for the print_progress function\n",
    "    \n",
    "    number_folders = len(data_folders) -1       # function to show the progress of loading and saving the data\n",
    "    current_folder_number = 0\n",
    "    notify_progress_threshold_out = notify_progress_threshold            # threshhold to update the progress every 10%\n",
    "        \n",
    "    return current_folder_number, number_folders, notify_progress_threshold_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BTo0KSjHg9F"
   },
   "outputs": [],
   "source": [
    "def print_progress(current_folder_number, number_folders, notify_progress_threshold):\n",
    "    # prints the progress of loading and preparing the data every 10%\n",
    "\n",
    "    current_folder_number += 1                  \n",
    "    progress = 100 * current_folder_number/number_folders\n",
    "    \n",
    "    if progress >= notify_progress_threshold:\n",
    "        \n",
    "        if notify_progress_threshold == 10:\n",
    "            \n",
    "            print('')\n",
    "            \n",
    "        print(str(notify_progress_threshold) + '%')\n",
    "        notify_progress_threshold += 10\n",
    "        \n",
    "    return current_folder_number, notify_progress_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9lr1BUaHgnq"
   },
   "outputs": [],
   "source": [
    "def get_folder_list(input_dir = '', output_dir = '', save_seperatly = False, save_all = False):\n",
    "    # gets a list of all folders in the input_dir and trys to create an outputfolder for the data is suppost\n",
    "    # to be saveed. Either with the given output_dir, or the function creates a folder for the output calles:\n",
    "    # raw_data_all/ for the output files.\n",
    "    \n",
    "    data_folders = glob.glob(input_dir + '*')   # gets a list of all folders in the input directory\n",
    "    \n",
    "    if save_all or save_seperatly:\n",
    "    \n",
    "        try:\n",
    "            \n",
    "            if output_dir:\n",
    "                \n",
    "                os.makedirs(output_dir)             # trying to make the passed output folder\n",
    "                \n",
    "                if save_all:\n",
    "                    \n",
    "                    os.makedirs(output_dir + 'raw_data_all/')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                output_dir = input_dir + 'raw_data/'\n",
    "                \n",
    "                if save_all:\n",
    "                    \n",
    "                    os.makedirs(output_dir + 'raw_data_all/')   \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    os.makedirs(output_dir) \n",
    "                    \n",
    "                        # trying to make the automatic output folder\n",
    "                \n",
    "            print('\\nCreation of the directory %s succesfull.' % output_dir)\n",
    "            \n",
    "        except OSError:\n",
    "            \n",
    "            print(\"\\nCreation of the directory %s failed.\" % output_dir)\n",
    "            \n",
    "    return data_folders, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07P7g19yHulh"
   },
   "outputs": [],
   "source": [
    "def get_measurement_data(data_folder = ''):\n",
    "    # loads the measurement data of the given subfolder which should contain the Accelerometer.csv and\n",
    "    # Gyroscope.csv file. After both are loaded in, the time vector of both is checked to allign the data.\n",
    "    # To achieve this the overhanging data is cut off and the data is allignd as close as possible.\n",
    "    # Finally the data of one of the sensors is interpolated so that the time vectors match and the data can\n",
    "    # be used reliably in the preprocessing. \n",
    "    # The output of this function is the trial information as DataFrames (gait, subject_number, trial) as well\n",
    "    # as the joined and alligned sensor data as a DataFrame.\n",
    "\n",
    "    folder_name = Path(data_folder).name\n",
    "    \n",
    "    empty_df = pd.DataFrame()\n",
    "    \n",
    "    if 'subject' not in folder_name:\n",
    "        \n",
    "        return empty_df, empty_df, empty_df, empty_df\n",
    "    \n",
    "    \n",
    "    subject_number_str = folder_name[7:10]\n",
    "    \n",
    "    if not subject_number_str[2].isdigit():\n",
    "            \n",
    "            subject_number_str = subject_number_str[0:2]\n",
    "    \n",
    "    \n",
    "    subject_number = pd.DataFrame(data = ([subject_number_str]), columns = ['subject_number'])\n",
    "    \n",
    "    trial_number = pd.DataFrame(data = ([folder_name[-2:]]), columns = ['trial_number'])\n",
    "        \n",
    "    data_accelerometer_dir = data_folder + '/Accelerometer.csv'\n",
    "    data_gyroscope_dir = data_folder + '/Gyroscope.csv'\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        data_accelerometer = pd.read_csv(data_accelerometer_dir)\n",
    "        data_gyroscope = pd.read_csv(data_gyroscope_dir)  \n",
    "    \n",
    "    except Exception:\n",
    "    \n",
    "        print('There was an Error loading the Data: %s' % data_folder)\n",
    "        \n",
    "        return empty_df, empty_df, empty_df, empty_df\n",
    "        \n",
    "    data_accelerometer = unify_measurement_label(data_accelerometer, data_accelerometer_dir)\n",
    "    data_gyroscope = unify_measurement_label(data_gyroscope, data_gyroscope_dir)\n",
    "\n",
    "    if data_accelerometer.empty or data_gyroscope.empty:\n",
    "        \n",
    "        return empty_df, empty_df, empty_df, empty_df\n",
    "\n",
    "    data_local = data_accelerometer.merge(data_gyroscope, left_on = 'Time [s]', right_on = 'Time [s]') # joins the data if the time vectors are aligned\n",
    "    data_local = data_local.reset_index()\n",
    "    data_local = data_local.drop(columns = 'index')\n",
    "    \n",
    "    if (not data_local.empty) and (not (data_local.iloc[0]['Time [s]'] == 0)):\n",
    "        \n",
    "        for i in range(data_local.iloc[:]['Time [s]'].size):\n",
    "            \n",
    "            data_local.at[i, 'Time [s]'] = data_local.iloc[i]['Time [s]'] - data_local.iloc[0]['Time [s]']\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#          \n",
    "    \n",
    "    if data_local.empty or ((data_local.size/data_local.columns.size) < (0.98 * data_accelerometer.size/data_accelerometer.columns.size)):\n",
    "\n",
    "        # the following code trys to allign the data of the accelerometer and gyroscope if they are misaligned\n",
    "        \n",
    "        frequency_accelerometer = get_frequency(data_accelerometer).iloc[0]['frequency [Hz]']\n",
    "        frequency_gyroscope = get_frequency(data_gyroscope).iloc[0]['frequency [Hz]']\n",
    "        \n",
    "        frequency_ratio = frequency_accelerometer/frequency_gyroscope\n",
    "        \n",
    "        offset_frequency = abs(1-frequency_ratio)     # computes the difference in freqeuncy\n",
    "\n",
    "        if (abs(2 - offset_frequency) < 0.05):\n",
    "            \n",
    "            data_accelerometer = data_accelerometer[0::2]\n",
    "            \n",
    "        elif (abs(3 - frequency_ratio) < 0.05):\n",
    "            \n",
    "            data_accelerometer = data_accelerometer[0::3]\n",
    "            \n",
    "        elif (abs(6 - frequency_ratio) < 0.05):\n",
    "            \n",
    "            data_accelerometer = data_accelerometer[0::6]\n",
    "            \n",
    "        elif (abs(0.5 - frequency_ratio) < 0.05):\n",
    "            \n",
    "            data_gyroscope = data_gyroscope[0::2]\n",
    "            \n",
    "        elif (abs(1/3 - frequency_ratio) < 0.05):\n",
    "            \n",
    "            data_gyroscope = data_gyroscope[0::3]\n",
    "            \n",
    "        elif (abs(1/6 - frequency_ratio) < 0.05):\n",
    "            \n",
    "            data_gyroscope = data_gyroscope[0::6]\n",
    "            \n",
    "            \n",
    "        frequency_accelerometer = get_frequency(data_accelerometer).iloc[0]['frequency [Hz]']\n",
    "        frequency_gyroscope = get_frequency(data_gyroscope).iloc[0]['frequency [Hz]']\n",
    "            \n",
    "        frequency_ratio = frequency_accelerometer/frequency_gyroscope\n",
    "    \n",
    "        if (abs(1 - frequency_ratio) > 0.05):\n",
    "                \n",
    "            print('The frequency of the acceloremeter and the gyroscope data of %s do not allign.' % folder_name)\n",
    "            \n",
    "            return empty_df, empty_df, empty_df, empty_df                # skips this measurement cause the frequencys are different\n",
    "        \n",
    "        iteration = 0\n",
    "        offset_time_min = 1\n",
    "        offset_time_iteration = 0\n",
    "        \n",
    "        if (data_gyroscope.iloc[0]['Time [s]'] - data_accelerometer.iloc[0]['Time [s]']) > 0:\n",
    "        \n",
    "            acc_starts = True       # means that the accelerometer measurement started before the gyroscope\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            acc_starts = False      # means that the gyroscope measurement started before the accelerometer\n",
    "        \n",
    "        for i in range(min(data_accelerometer.iloc[:]['Time [s]'].size, data_gyroscope.iloc[:]['Time [s]'].size)): \n",
    "            \n",
    "        # This loop computes the datapoint of the first starting sensor which has the lowest time distance to the starting\n",
    "        # datapoint of the sensor which is starting first\n",
    "            \n",
    "            if acc_starts:\n",
    "            \n",
    "                offset_time_iteration = abs(data_gyroscope.iloc[0]['Time [s]'] - data_accelerometer.iloc[i]['Time [s]'])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                offset_time_iteration = abs(data_accelerometer.iloc[0]['Time [s]'] - data_gyroscope.iloc[i]['Time [s]'])\n",
    "            \n",
    "            offset_time_min = min(offset_time_min, offset_time_iteration)\n",
    "            \n",
    "            if offset_time_min == offset_time_iteration:\n",
    "                    \n",
    "                iteration = i\n",
    "                \n",
    "            elif offset_time_min < offset_time_iteration:\n",
    "                \n",
    "                break\n",
    "            \n",
    "        offset_datapoints = data_accelerometer.iloc[:]['Time [s]'].size - data_gyroscope.iloc[:]['Time [s]'].size\n",
    "        \n",
    "        # obove the difference in datapoints of the accelerometer and gyroscope are computed \n",
    "        \n",
    "        if offset_datapoints > 0:\n",
    "            \n",
    "            for i in range(iteration):  # this loop erases all measurements of the first starting sensor that are \n",
    "                                        # previous to the starting point of the later starting sensor\n",
    "                \n",
    "                if acc_starts: \n",
    "                \n",
    "                    data_accelerometer = data_accelerometer.drop([i])\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    data_gyroscope = data_gyroscope.drop([i])                        \n",
    "                    \n",
    "            offset_datapoints_new = data_accelerometer.iloc[:]['Time [s]'].size - data_gyroscope.iloc[:]['Time [s]'].size\n",
    "            \n",
    "            data_accelerometer = data_accelerometer.reset_index()\n",
    "            data_accelerometer = data_accelerometer.drop(columns = 'index')\n",
    "            data_gyroscope = data_gyroscope.reset_index()\n",
    "            data_gyroscope = data_gyroscope.drop(columns = 'index')    \n",
    "            \n",
    "            if not offset_datapoints_new == 0:\n",
    "                \n",
    "                for i in range(abs(offset_datapoints_new)):     # this loop erases all measurements of the later stopping\n",
    "                                                                # sensor that are after the shorter recording sensor stops\n",
    "                    \n",
    "                    if offset_datapoints_new > 0:\n",
    "                            \n",
    "                        data_accelerometer = data_accelerometer.drop([data_accelerometer.iloc[:]['Time [s]'].size - 1])\n",
    "                            \n",
    "                    else:\n",
    "                            \n",
    "                        data_gyroscope = data_gyroscope.drop([data_gyroscope.iloc[:]['Time [s]'].size - 1])  \n",
    "\n",
    "\n",
    "        \n",
    "        data_accelerometer = data_accelerometer.reset_index()\n",
    "        data_accelerometer = data_accelerometer.drop(columns = 'index')\n",
    "        data_gyroscope = data_gyroscope.reset_index()\n",
    "        data_gyroscope = data_gyroscope.drop(columns = 'index')    \n",
    "        \n",
    "        data_accelerometer_time = pd.DataFrame(data = np.array(data_accelerometer['Time [s]']), columns = ['Time [s]'])\n",
    "        data_gyroscope_time = pd.DataFrame(data = np.array(data_gyroscope['Time [s]']), columns = ['Time [s]'])\n",
    "\n",
    "        data_time_merged = data_gyroscope_time.append(data_accelerometer_time)\n",
    "        data_time_merged = data_time_merged.reset_index()\n",
    "        data_time_merged = data_time_merged.drop(columns = 'index')\n",
    "    \n",
    "        data_gyroscope = data_gyroscope.drop(columns = 'Time [s]')\n",
    "        data_gyroscope = data_time_merged.join(data_gyroscope)\n",
    "        data_gyroscope = data_gyroscope.drop_duplicates(subset = ['Time [s]'])\n",
    "        data_gyroscope = data_gyroscope.sort_values(by = ['Time [s]'])\n",
    "        data_gyroscope = data_gyroscope.interpolate(method='linear', axis = 0)\n",
    "        \n",
    "        \n",
    "        data_local = data_accelerometer.merge(data_gyroscope, left_on = 'Time [s]', right_on = 'Time [s]')\n",
    "        data_local = data_local.reset_index()\n",
    "        data_local = data_local.drop(columns = 'index')\n",
    "        \n",
    "        for i in range(data_local.iloc[:]['Time [s]'].size):    \n",
    "            \n",
    "            data_local.at[i, 'Time [s]'] = data_local.iloc[i]['Time [s]'] - data_local.iloc[0]['Time [s]']\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#       \n",
    "        \n",
    "    if 'normal' in data_folder:             # gets the gait (normal or impaired) out of the folder name\n",
    "            \n",
    "        gait = pd.DataFrame(data=([0]), columns = ['gait'])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        gait = pd.DataFrame(data=([1]), columns = ['gait'])\n",
    "        \n",
    "    data_local = data_local.join(gait).join(subject_number).join(trial_number)\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#   \n",
    "    \n",
    "    return data_local, subject_number, trial_number, gait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDeSOPO5Hu4n"
   },
   "outputs": [],
   "source": [
    "def get_join_anthropometric_data(data_local, data_folder, subject_number, anthropology_file_dir = ''):\n",
    "  # This functino loads the anthropometric data file either from the anthropology_file_dir if one is defined,\n",
    "  # or if not it trys to load the file from the parent folder of the input_dir.\n",
    "  # After loading all the data the anthropometric information of the current subject is loaded and unified (\n",
    "  # male, M, man -> 1, etc.). Then it is added to the sensor data DataFrame. This DataFrame is then returned\n",
    "  # from the function.\n",
    "\n",
    "    if not anthropology_file_dir:\n",
    "    \n",
    "        anthropology_file_dir = str(Path(data_folder).parent.parent) + '/anthro.xlsx'\n",
    "        \n",
    "    anthropometric_data_all = pd.read_excel(anthropology_file_dir)\n",
    "    \n",
    "    for subject_number_anthro in anthropometric_data_all.iloc[:]['subject no.']:\n",
    "\n",
    "        if int(str(subject_number_anthro)[1:]) == int(subject_number.iloc[0]['subject_number']):\n",
    "                \n",
    "            anthropometric_data_list = anthropometric_data_all.iloc[int(str(subject_number_anthro)[1:])-1][:]\n",
    "            anthropometric_data = pd.DataFrame(anthropometric_data_list)\n",
    "            \n",
    "            anthropometric_data = anthropometric_data.transpose()\n",
    "            anthropometric_data = anthropometric_data.reset_index()\n",
    "   \n",
    "            anthropometric_data = anthropometric_data.drop(columns = 'subject no.').drop(columns = 'index')\n",
    "            anthropometric_data = anthropometric_data.rename(columns = {'sex': 'subject_sex', 'age': 'subject_age [y]',\n",
    "                                                                        'height [m]': 'subject_height [m]', \n",
    "                                                                        'weight [kg]': 'subject_weight [kg]'})\n",
    "                                       \n",
    "            if anthropometric_data.empty:\n",
    "                \n",
    "                return pd.DataFrame()\n",
    "    \n",
    "            anthropometric_data = unify_anthropometric_data(anthropometric_data)\n",
    "            \n",
    "            if not anthropometric_data.empty:\n",
    "\n",
    "                data_local = data_local.join(anthropometric_data)   # adding the anthropology data to the rest of the dataframe\n",
    "            \n",
    "                break\n",
    "            \n",
    "            else: \n",
    "                \n",
    "                return pd.DataFrame()\n",
    "        \n",
    "    return data_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DshLagsL2kii"
   },
   "outputs": [],
   "source": [
    "def load_join_label(input_dir='', output_dir='', anthropology_file_dir = '', \n",
    "                    save_seperatly = False, save_all = False, return_data = True):\n",
    "    \n",
    "    if not input_dir:       # checking if the string is empty\n",
    "        \n",
    "        print('\\nError: string of input directory is empty.')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    if return_data:\n",
    "        \n",
    "        data_all = pd.DataFrame()\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    data_folders, output_dir = get_folder_list(input_dir, output_dir, save_seperatly, save_all)\n",
    "        \n",
    "    current_folder_number, number_folders, notify_progress_threshold = initialize_print_progress(data_folders, notify_progress_threshold = 10)\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    for data_folder in data_folders:                # loading the accelerometer and gyroscape data, joining and labeling it\n",
    "        \n",
    "        current_folder_number, notify_progress_threshold = print_progress(current_folder_number, number_folders, notify_progress_threshold)\n",
    "        \n",
    "        data_local, subject_number, trial_number, gait = get_measurement_data(data_folder)\n",
    "        \n",
    "        if data_local.empty:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        data_local = get_join_anthropometric_data(data_local, data_folder, subject_number, anthropology_file_dir)\n",
    "        \n",
    "        if data_local.empty:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        frequency = get_frequency(data_local)       # getting the frequency of the data\n",
    "        data_local = data_local.join(frequency)     # adding the frequency to the rest of the dataframe\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#            \n",
    "            \n",
    "        if return_data:   # adding the current data to the matrix with all data\n",
    "            \n",
    "            if data_all.empty:\n",
    "                \n",
    "              data_all = data_local\n",
    "\n",
    "            else:\n",
    "\n",
    "              data_all = data_all.append(data_local, ignore_index = True, sort=False)\n",
    "     \n",
    "        if save_seperatly:          # saving the data as csv in the output directory\n",
    "        \n",
    "            data_local.to_csv(output_dir + 'raw_data_subject' + str(subject_number.iloc[0]['subject_number']) + '_gait' \n",
    "                              + str(data_local.iloc[0]['gait'])[0] + '_trial' + str(trial_number.iloc[0]['trial_number']) + '.csv')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#         \n",
    "        \n",
    "    if save_seperatly or save_all:\n",
    "        \n",
    "        if save_all:\n",
    "            \n",
    "            data_all.to_csv(output_dir + 'raw_data_all/raw_data_all.csv')\n",
    "        \n",
    "        print('\\nThe raw data was succesfully saved in: %s.' % output_dir)    \n",
    "    \n",
    "    if return_data:\n",
    "\n",
    "        return data_all\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evMM22g_2kih"
   },
   "outputs": [],
   "source": [
    "data_all_prepared = load_join_label(input_dir = input_dir, output_dir = output_dir, save_seperatly = True, save_all = False, return_data = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycJIyuioQIti"
   },
   "source": [
    "# 2. Data preprocessing\n",
    "\n",
    "Input is a dataframe which was created in the first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YHRHnQfQpmM"
   },
   "outputs": [],
   "source": [
    "path_to_raw_data = '/content/drive/MyDrive/Colab Notebooks/CIE Data/raw_data/'\n",
    "output_directory=\"../Smartphone2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckgBuh-5rn4V"
   },
   "source": [
    "## Filter Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNkHtwFJrqJA"
   },
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    # This function applies the Butterworth filter\n",
    "    # Input: data: The data to be filtered\n",
    "    #        fs: Sample frequency\n",
    "    #        lowcut, highcut: Desired cutoff frequencies (in Hz)\n",
    "    #        order: The order of the filter: Transmission loss of order*20 dB per frequency decade\n",
    "    # Output: y: filtered data \n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n",
    "    y = sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "def butter_bandpass_DataFrame(data, lowcut_acc=1, lowcut_gyr=0.25, highcut_acc=20, highcut_gyr=30, order=4):\n",
    "    # Applies the Butterworth filter for a whole DataFrame\n",
    "    # Input: data: DataFrame to be filtered \n",
    "    #        lowcut_acc, lowcut_gyr, highcut_acc, highcut_gyr: Desired cutoff frequencies (in Hz) for accelerometer and gyroscope\n",
    "    #        order: The order of the filter: Transmission loss of order*20 dB per frequency decade\n",
    "    # Output: data: Filtered DataFrame\n",
    "    col = data.columns\n",
    "    fs = data.iloc[0]['frequency [Hz]']\n",
    "\n",
    "    X_acc = data[col[2]].to_numpy()\n",
    "    Y_acc = data[col[3]].to_numpy()\n",
    "    Z_acc = data[col[4]].to_numpy()\n",
    "    X_gyr = data[col[5]].to_numpy()\n",
    "    Y_gyr = data[col[6]].to_numpy()\n",
    "    Z_gyr = data[col[7]].to_numpy()\n",
    "    \n",
    "    X_acc_filtered = butter_bandpass_filter(X_acc, lowcut_acc, highcut_acc, fs, order)\n",
    "    Y_acc_filtered = butter_bandpass_filter(Y_acc, lowcut_acc, highcut_acc, fs, order)\n",
    "    Z_acc_filtered = butter_bandpass_filter(Z_acc, lowcut_acc, highcut_acc, fs, order)\n",
    "    X_gyr_filtered = butter_bandpass_filter(X_gyr, lowcut_gyr, highcut_gyr, fs, order)\n",
    "    Y_gyr_filtered = butter_bandpass_filter(Y_gyr, lowcut_gyr, highcut_gyr, fs, order)\n",
    "    Z_gyr_filtered = butter_bandpass_filter(Z_gyr, lowcut_gyr, highcut_gyr, fs, order)\n",
    "    \n",
    "    data.loc[:]['acceleration_x [m/s^2]'] = X_acc_filtered\n",
    "    data.loc[:]['acceleration_y [m/s^2]'] = Y_acc_filtered\n",
    "    data.loc[:]['acceleration_z [m/s^2]'] = Z_acc_filtered\n",
    "    data.loc[:]['angular_velocity_x [rad/s]'] = X_gyr_filtered\n",
    "    data.loc[:]['angular_velocity_y [rad/s]'] = Y_gyr_filtered\n",
    "    data.loc[:]['angular_velocity_z [rad/s]'] = Z_gyr_filtered\n",
    "\n",
    "    return data\n",
    "\n",
    "def savitzky_gola(data, window_length, polyorder):\n",
    "    # This function applies the Savitzky_gola filter\n",
    "    # Input: data: The data to be filtered\n",
    "    #        window_length: The length of the filter window(the number of coefficients): Must be odd\n",
    "    #        polyorder: The order of the polynomial used to fit the samples: Must be < window_length\n",
    "    # Output: y: filtered data\n",
    "    y = signal.savgol_filter(data, window_length, polyorder, mode = 'nearest', axis=0) #window length: odd positive integer, polyorder < window length\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VTRmzlcrqsm"
   },
   "source": [
    "## Extract Motion Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfaFAONJsbrr"
   },
   "outputs": [],
   "source": [
    "def remove_beginning_and_end(data, standard_deviation_threshold = 1):\n",
    "    #   This function automatically detects the start and end of the walking area and removes the non-relevant data before the start and after the end\n",
    "    #   Input:      data: Data in form of a pd.DataFrame and a \n",
    "    #               standard_deviation_threshold: Threshold for the standard deviation at which the data are to be considered malicious\n",
    "    #   Output:     data: Data with removed beginning and end\n",
    "    try:\n",
    "        print('\\nData set: gait '+ str(data.iloc[0]['gait'])+ ', subject_number ' + str(data.iloc[0]['subject_number']) + ', trial_number ' + str(data.iloc[0]['trial_number']))\n",
    "        \n",
    "        # Import acceleration in z direction and time vector\n",
    "        col = data.columns\n",
    "        T_acc = data[col[1]].to_numpy()\n",
    "        Z_acc = data[col[4]].to_numpy()\n",
    "        \n",
    "        # Remove the offset of the z acceleration\n",
    "        offset = np.mean(Z_acc)\n",
    "        Z_acc = Z_acc - offset\n",
    "\n",
    "        # Sample rate and desired cutoff frequencies (in Hz) for Butterworth filter.\n",
    "        fs = data.iloc[0]['frequency [Hz]']\n",
    "        lowcut = 0.5\n",
    "        highcut = 3\n",
    "        \n",
    "        # Apply Butterworth filter \n",
    "        Z_acc_filter = butter_bandpass_filter(Z_acc, lowcut, highcut, fs, order=5)\n",
    "\n",
    "        # Assumed interval of one step in seconds. Assumed to be 1 second. If 1s proves to be insufficient, it can be adjusted later.\n",
    "        interv = 1\n",
    "\n",
    "        # Approximating number of timesteps for one interval\n",
    "        intlength = int(interv*fs)\n",
    "\n",
    "        # Find peaks for beginning and end detection\n",
    "        peaks, _ = signal.find_peaks (Z_acc_filter, distance = intlength)\n",
    "        peaks_unfiltered, _ = signal.find_peaks (Z_acc, distance = intlength)\n",
    "        \n",
    "        # Arrays to find beginning and end\n",
    "        zmax = Z_acc_filter[peaks] # List of all amplitudes of the peaks found in the filtered data\n",
    "        middle = int(len(Z_acc_filter)/2) # Middle of the measuring points\n",
    "        zmax_middle = abs(np.array(peaks)-middle).argmin() # Corresponding middle in zmax\n",
    "        middle_area = zmax[zmax_middle - 7 : zmax_middle + 8] # Array of zmax for an area in the middle: In this area the person walks in any case                                              \n",
    "        standard_deviation = np.std(middle_area) # The average amplitude of the middle area\n",
    "        average_zmax = np.mean(middle_area) # The standard deviation of the amplitude of the middle section                                              \n",
    "        divergent = [zmax[i] > average_zmax - standard_deviation * 4 and zmax[i] < average_zmax + standard_deviation * 4 for i in range(len(zmax))] # Does the amplitude of the peak of the filtered data deviate from the mean value of the middle area only within static limits\n",
    "        zmax_unfiltered = Z_acc[peaks_unfiltered] # List of all amplitudes of the peaks found in the  unfiltered data\n",
    "        average_zmax_unfiltered = np.mean(zmax_unfiltered) \n",
    "        mean_unfiltered = np.mean(Z_acc[middle - 7 * intlength : middle + 7 * intlength]) # The mean value of the unfiltered data for an area in the middle \n",
    "        \n",
    "        # Definition of limits that are later used to detect peaks with small aplitude\n",
    "        if average_zmax > 3:\n",
    "            limit = 1.2\n",
    "        else:\n",
    "            limit = 0.69\n",
    "            \n",
    "        # Definition of the time threshold\n",
    "        time_threshold = int(intlength/2)\n",
    "\n",
    "        # Decide whether the standard deviation is too large compared to the specified standard deviation threshold\n",
    "        if standard_deviation > standard_deviation_threshold:\n",
    "            print('\\nThe data set: gait '+ str(data.iloc[0]['gait'])+ ', subject_number ' + str(data.iloc[0]['subject_number']) + ', trial_number ' + str(data.iloc[0]['trial_number']) + ' is removed.')\n",
    "            data = pd.DataFrame()\n",
    "        else:  \n",
    "            \n",
    "            ## Find beginning\n",
    "\n",
    "            # Find large amplitude changes of the peaks of the filtered signal compared to a region in the middle\n",
    "            j = zmax_middle\n",
    "            while j > 0:\n",
    "                if divergent[j] == False: \n",
    "                    beginning = peaks[j+1]\n",
    "                    break\n",
    "                else:\n",
    "                    j -= 1     \n",
    "            if j == 0 and (Z_acc[peaks_unfiltered[0]] > 7.5 or Z_acc[peaks_unfiltered[1]] > 7.5):\n",
    "                j = 1       \n",
    "            beginning = peaks[j]\n",
    "\n",
    "            # Find peaks of the filtered signal with really small aplitude close to zero\n",
    "            for i in range(zmax_middle):\n",
    "                if zmax[i] > -limit and zmax[i] < limit and beginning < peaks[i+1]:\n",
    "                    beginning = peaks[i+1] \n",
    "            \n",
    "            # Find areas where the amplitude is always below an amplitude limit for a given time or areas where the local average is much higher or lower than the average for an area in the middle\n",
    "            area_zero = False\n",
    "            l = int(len(Z_acc)/2)\n",
    "            if beginning > 2*time_threshold:\n",
    "                start = beginning + time_threshold\n",
    "            else:\n",
    "                start = beginning - time_threshold\n",
    "            while l > start:\n",
    "                if len(Z_acc[int(l-time_threshold):l]) == time_threshold:\n",
    "                    mean = np.mean(Z_acc[int(l-time_threshold):l])\n",
    "                else: \n",
    "                    mean = np.mean(Z_acc[0:l])\n",
    "                if mean > average_zmax* 1.3 + mean_unfiltered or mean < -average_zmax*1.3 + mean_unfiltered:\n",
    "                    difference = True\n",
    "                    l = int(l+4*time_threshold)\n",
    "                else:\n",
    "                    difference = False\n",
    "                for m in range(int(l-time_threshold),l):\n",
    "                    if Z_acc[m] > -average_zmax_unfiltered/6 and Z_acc[m] < average_zmax_unfiltered/6 or difference == True:\n",
    "                        area_zero = True\n",
    "                        continue\n",
    "                    else:\n",
    "                        area_zero = False\n",
    "                        break\n",
    "                if area_zero == False:\n",
    "                    l = int(l-time_threshold)\n",
    "                else:\n",
    "                    next_peak = abs(np.array(peaks)-l).argmin()\n",
    "                    if peaks[next_peak] < l:\n",
    "                        beginning = peaks[next_peak+1]\n",
    "                    else:\n",
    "                        beginning = peaks[next_peak]\n",
    "                    break\n",
    "                   \n",
    "            ## Find ending\n",
    "\n",
    "            # Find large amplitude changes of the peaks of the filtered signal compared to a region in the middle\n",
    "            k = zmax_middle\n",
    "            while k < len(divergent) - 1:\n",
    "                if divergent[k] == False:\n",
    "                    ending = peaks[k-1]\n",
    "                    break\n",
    "                else:\n",
    "                    k += 1     \n",
    "            if k == len(divergent) - 1 and (Z_acc[peaks_unfiltered[len(peaks_unfiltered)-1]] > 7.5 or Z_acc[peaks_unfiltered[len(peaks_unfiltered)-2]] > 7.5):\n",
    "                k = len(divergent) - 2    \n",
    "            ending = peaks[k]\n",
    "            \n",
    "            # Find peaks of the filtered signal with really small aplitude close to zero\n",
    "            for i in range(zmax_middle, len(zmax)-1):\n",
    "                if zmax[i] > -limit and zmax[i] < limit and ending > peaks[i-1]:\n",
    "                    ending = peaks[i-1]\n",
    "                    break\n",
    "\n",
    "            # Find areas where the amplitude is always below an amplitude limit for a given time or areas where the local average is much higher or lower than the average for an area in the middle\n",
    "            area_zero = False     \n",
    "            l = int(len(Z_acc)/2)\n",
    "            if ending < len(Z_acc) - 2*time_threshold:\n",
    "                end = ending + time_threshold\n",
    "            else:\n",
    "                end = ending - time_threshold\n",
    "            while l < end:\n",
    "                if len(Z_acc[l:int(l+time_threshold)]) == time_threshold:\n",
    "                    mean = np.mean(Z_acc[l:int(l+time_threshold)])\n",
    "                else: \n",
    "                    mean = np.mean(Z_acc[l:len(Z_acc)-1])\n",
    "                if mean > average_zmax*1.3+mean_unfiltered or mean < -average_zmax*1.3+ mean_unfiltered:\n",
    "                    difference = True\n",
    "                    l = int(l-4*time_threshold)\n",
    "                else:\n",
    "                    difference = False\n",
    "                for m in range(l,int(l+time_threshold)):\n",
    "                    if Z_acc[m] > -average_zmax_unfiltered/6 and Z_acc[m] < average_zmax_unfiltered/6 or difference == True:\n",
    "                        area_zero = True\n",
    "                        continue\n",
    "                    else:\n",
    "                        area_zero = False\n",
    "                        break\n",
    "                if area_zero == False:\n",
    "                    l = int(l+time_threshold)\n",
    "                else:\n",
    "                    next_peak = abs(np.array(peaks)-l).argmin()\n",
    "                    if peaks[next_peak] < l:\n",
    "                        ending = peaks[next_peak]\n",
    "                    else:\n",
    "                        ending = peaks[next_peak-1]\n",
    "                    break\n",
    "            \n",
    "            # If the found beginning is also the found end, the data set is removed\n",
    "            if beginning==ending:\n",
    "                print('\\nThe data set: gait '+ str(data.iloc[0]['gait'])+ ', subject_number ' + str(data.iloc[0]['subject_number']) + ', trial_number ' + str(data.iloc[0]['trial_number']) + ' is removed.')\n",
    "                data = pd.DataFrame()\n",
    "            \n",
    "            # Remove all data up to the found beginning and from the found end and save the information from the first line into the new first line\n",
    "            else:                \n",
    "                gait = data.loc[0,('gait')]\n",
    "                subject_number = data.loc[0,('subject_number')]\n",
    "                trial_number = data.loc[0,('trial_number')]\n",
    "                subject_sex = data.loc[0,('subject_sex')]\n",
    "                subject_age = data.loc[0,('subject_age [y]')]\n",
    "                subject_height = data.loc[0,('subject_height [m]')]\n",
    "                subject_weight = data.loc[0,('subject_weight [kg]')]\n",
    "                frequency = data.loc[0,('frequency [Hz]')]  \n",
    "                \n",
    "                for i in range(beginning):\n",
    "                    data= data.drop([i])\n",
    "                for i in range(ending+1,len(Z_acc)):\n",
    "                    data = data.drop([i]) \n",
    "                \n",
    "                data.loc[beginning,('gait')]= gait\n",
    "                data.loc[beginning,('subject_number')] = subject_number\n",
    "                data.loc[beginning,('trial_number')] =  trial_number\n",
    "                data.loc[beginning,('subject_sex')] = subject_sex\n",
    "                data.loc[beginning,('subject_age [y]')] = subject_age\n",
    "                data.loc[beginning,('subject_height [m]')] = subject_height\n",
    "                data.loc[beginning,('subject_weight [kg]')] = subject_weight\n",
    "                data.loc[beginning,('frequency [Hz]')] = frequency\n",
    "                \n",
    "        \n",
    "    except Exception:\n",
    "            \n",
    "        print('\\nFailed to remove the beginning and end of the data set: gait ' + str(data.iloc[0]['gait'])+ ', subject_number ' + str(data.iloc[0]['subject_number']) + ', trial_number ' + str(data.iloc[0]['trial_number']))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucNriKYTrtej"
   },
   "source": [
    "## Rotate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUxjYgM1Rz4j"
   },
   "outputs": [],
   "source": [
    "def rotate_Data(data):\n",
    "    #   This function calculates the gravity vector through the mean of the \n",
    "    #   acceleration data. It then performs a Principle component analysis in \n",
    "    #   order to rotate the g-vector into the x-axis. To eliminate the last DOF\n",
    "    #   the data will be rotated around the x-axis to maximize the angular \n",
    "    #   velocity. (Criterion for max angular velocity is of emperical nature. It \n",
    "    #   seems to obtain the most uniform results.)\n",
    "    #\n",
    "    #   Input:      data: pd.DataFrame    unrotated Data\n",
    "    #   Output:     data: pd.DataFrame    rotated Data\n",
    " \n",
    "\n",
    "    # from DataFame to array\n",
    "    col = data.columns\n",
    "    acc = data[[col[2], col[3], col[4]]].to_numpy()\n",
    "    gyro = data[[col[5], col[6], col[7]]].to_numpy()\n",
    "\n",
    "    # get mean of accaleration components (should be gravitational accerlation as a vector)\n",
    "    accmean = np.mean(acc, axis=0)\n",
    "\n",
    "    # perform a singular value decomposition on g vector to get rotation matrix \n",
    "    M = np.outer(accmean.T, accmean)\n",
    "    u, s, vh = np.linalg.svd(M)\n",
    "\n",
    "    # rotate all the Data by rotation matrix from previous step\n",
    "    acc = np.matmul(acc, u)\n",
    "    gyro = np.matmul(gyro, u)\n",
    "\n",
    "    # calculate updated g vector. should only now be in x-direction\n",
    "    accmean1 = np.mean(acc, axis=0)\n",
    "\n",
    "    # if g vector is in negative x-direction flip coordinate system 180 degrees around z axis, so it alwas points in positive x-direction\n",
    "    if accmean1[0] < 0:\n",
    "        r = R.from_euler('z', 180, degrees=True).as_matrix()\n",
    "        acc = np.matmul(acc, r)\n",
    "        gyro = np.matmul(gyro, r)\n",
    "\n",
    "    # After pca of g-vector, there is still 1 degree of freedom (rotation around new x-axis)\n",
    "    # rotate around x-axis so that angular velocity around z-axis gets maximized\n",
    "    maxy = []\n",
    "    gyro1 = gyro\n",
    "    #rotate to find maximum value\n",
    "    for i in range(0,360):\n",
    "        r = R.from_euler('x', 1, degrees=True).as_matrix()\n",
    "        gyro1 = np.matmul(gyro1, r)\n",
    "        maxy.append(gyro1[:, 2].max())\n",
    "\n",
    "    angle = max(maxy)\n",
    "    angle = maxy.index(angle)\n",
    "\n",
    "    # Rotate around found angle\n",
    "    r = R.from_euler('x', angle, degrees=True).as_matrix()\n",
    "    acc = np.matmul(acc, r)\n",
    "    gyro = np.matmul(gyro, r)\n",
    "\n",
    "    # write updated data back into DataFrame\n",
    "    data[[col[2], col[3], col[4]]] = acc\n",
    "    data[[col[5], col[6], col[7]]] = gyro\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxYC_45ErwCi"
   },
   "source": [
    "## Segment Trial into Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wD2DXiDir1ML"
   },
   "outputs": [],
   "source": [
    "def getsteps(data, plotme=False):\n",
    "    # This function tries to detect the starting positions of each step.\n",
    "    # This function defines the starting point at the peaks of the acceleration \n",
    "    # data in x-direction. x-Direction is chosen since rotation yields the \n",
    "    # highest confidence and therefore regularity in this direction.\n",
    "    #\n",
    "    # input:  data:     pd.DataFrame with gait data.\n",
    "    #         plotme:   boolean; Creates a plot if set to True. False by default\n",
    "    # output: stepidx:  Array with indices of the starting points of the steps\n",
    "\n",
    "    col = data.columns\n",
    "\n",
    "    # Data to arrays\n",
    "    t = data[col[1]].to_numpy()\n",
    "    x = data[col[2]].to_numpy()\n",
    "\n",
    "    # Filtering the data seems to improve the consitency in finding peaks, yet \n",
    "    # it does not change the location of them.\n",
    "    # Sample rate and desired cutoff frequencies (in Hz) for Butterworth filter.\n",
    "    fs = data.iloc[0]['frequency [Hz]']\n",
    "    lowcut = 0.5\n",
    "    highcut = 4\n",
    "\n",
    "    # Apply Butterworth filter\n",
    "    x = butter_bandpass_filter(x, lowcut, highcut, fs, order=5)\n",
    "\n",
    "    # Assumed duration of one step in seconds.\n",
    "    interv = 1\n",
    "\n",
    "    # Approximating number of timesteps for one interval\n",
    "    intlength = (np.abs(t - (interv+t[1]))).argmin()-1\n",
    "\n",
    "    # Initializing output array\n",
    "    stepidx = []\n",
    "    diff = []\n",
    "\n",
    "    # plot for visualization\n",
    "    if plotme:\n",
    "        plt.figure()\n",
    "        plt.plot(t, x)\n",
    "\n",
    "    # Defining start end endpoint of first search interval\n",
    "    int_start = 0\n",
    "    int_end = int(round(int_start + intlength))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Loop until the end. (i<100 as safty feature for while-loop)\n",
    "    while int_end < len(x) and i < 100:\n",
    "\n",
    "        # find maximum in interval\n",
    "        stepidx.append(int_start + x[int_start: int_end].argmax())\n",
    "        diff.append(stepidx[i]-stepidx[i-1])\n",
    "\n",
    "        # update interval\n",
    "        int_start = int(round(stepidx[i] + round(0.5*intlength)))\n",
    "        int_end = int(round(int_start + round(1*intlength)))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Evaluate quality of result.\n",
    "    diff[0] = stepidx[0]\n",
    "    diffstd = np.std(diff[1:-1])\n",
    "    diffmed = int(np.median(diff))\n",
    "\n",
    "    # If standard deviation between step lengths is too high, run once more with \n",
    "    # median steplength as better step length approximation. Interval where \n",
    "    # algorithm is looking for peak can now be narrower.\n",
    "    if diffstd > 0.1*diffmed:\n",
    "\n",
    "        # Updated search parameters\n",
    "        intlength = diffmed\n",
    "        int_start = int(round(x.argmax() % intlength))\n",
    "        int_end = int(round(int_start + intlength))\n",
    "\n",
    "        stepidx2 = []\n",
    "        diff2 = []\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        # Loop until the end. (i<100 as safty feature for while-loop)\n",
    "        while int_end < len(x) and i < 100:\n",
    "            # find maximum in interval\n",
    "            stepidx2.append(int_start + x[int_start:int_end].argmax())\n",
    "            diff2.append(stepidx2[i] - stepidx2[i - 1])\n",
    "\n",
    "            # update interval\n",
    "            int_start = int(round(stepidx2[i] + (diffmed - 0.5 * diffstd)))\n",
    "            int_end = int(round(int_start + (diffstd + 0.5 * diffstd)))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # plot for visualization\n",
    "        if plotme:\n",
    "            plt.plot(t[stepidx2], x[stepidx2], 'x', c='g')\n",
    "\n",
    "        return stepidx2\n",
    "\n",
    "    if plotme:\n",
    "        plt.plot(t[stepidx], x[stepidx], 'x', c='r')\n",
    "\n",
    "    return stepidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDCLwBPOr1tN"
   },
   "source": [
    "## Resampling and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiU-g72Dr5O7"
   },
   "outputs": [],
   "source": [
    "def resample(data, stepidx, step_length): \n",
    "    \n",
    "    # Function for resampling data to same frame number\n",
    "    # input arguments: data (acquired after rotate_Data() function)\n",
    "    #                  stepidx (array retured from getsteps() function)\n",
    "    #                  step_length (number of required data points for each step)\n",
    "    #                  output_dir (directory to store the resulting resampled data files)\n",
    "    # output: All_steps (contains data for each step)\n",
    "    \n",
    "\n",
    "    col=data.columns\n",
    "    data = data.fillna(method ='ffill') \n",
    "    size = len(stepidx)                 # to get number of steps\n",
    "    number = step_length                # 100: fixed value for number of required data points for each step\n",
    "    d = pd.DataFrame()\n",
    "    dnew = pd.DataFrame()\n",
    "    All_steps = pd.DataFrame()\n",
    "    \n",
    "    for i in range(size-1):\n",
    "                                            \n",
    "        d = pd.DataFrame(data).set_index(data[col[0]])[stepidx[i]:stepidx[i+1]].copy(deep=True)     # separating each step\n",
    "        d = sp.signal.resample(d,number)                                                            # resampling\n",
    "       \n",
    "    \n",
    "        # resampling converts dataframe to numpy array\n",
    "        # next step converts the obtained numpy array to dataframe again        \n",
    "        \n",
    "        \n",
    "        dnew = pd.DataFrame(data=d, columns = ['steps_data_points', 'Time [s]','acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]'\n",
    "                                               , 'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]', 'gait',\n",
    "                                               'subject_number', 'trial_number', 'subject_sex', 'subject_age [y]',\n",
    "                                               'subject_height [m]', 'subject_weight [kg]', 'frequency [Hz]'])\n",
    "        \n",
    "        \n",
    "        # Normalization of the sensors data onto [-1,1]\n",
    "        \n",
    "        cols_to_norm = ['acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]', 'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]']\n",
    "        \n",
    "        dnew[cols_to_norm] = dnew[cols_to_norm].apply(lambda x: 2*((x - x.min()) / (x.max() - x.min()))-1)\n",
    "\n",
    "        # Normalization of the anthromopetric data onto [0,1]\n",
    "\n",
    "        age_norm = ['subject_age [y]']\n",
    "        height_norm = ['subject_height [m]']\n",
    "        weight_norm = ['subject_weight [kg]']\n",
    "        \n",
    "        dnew[age_norm] = dnew[age_norm].apply(lambda x: ((x - 21) / (47 - 21)))\n",
    "        dnew[height_norm] = dnew[height_norm].apply(lambda x: ((x - 1.55) / (2.0 - 1.55)))\n",
    "        dnew[weight_norm] = dnew[weight_norm].apply(lambda x: ((x - 46) / (200 - 46)))\n",
    "\n",
    "        del dnew['steps_data_points']\n",
    "\n",
    "        dnew.insert(0, 'step_number', i+1)\n",
    "        \n",
    "        All_steps = All_steps.append(dnew, ignore_index = True)\n",
    "    \n",
    "        \n",
    "    return All_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW1ObNIZr5vf"
   },
   "source": [
    "## Check and remove malicious sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enebiz_BIaut"
   },
   "outputs": [],
   "source": [
    "def initialize_plot_all_steps_of_one_trial(trial_information = ''): \n",
    "    # takes the information of the trial as input (subject number, gait, trial)\n",
    "    # creates and returns a fig subplot for plotting the steps of one trial\n",
    "        \n",
    "    fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(20,10))\n",
    "    fig.suptitle('Data of one trial\\n' + trial_information, fontsize = 20)\n",
    "    \n",
    "    axs[0, 0].set_title('acceleration_x [m/s^2]')\n",
    "    axs[0, 0].set_ylim(-1,1)\n",
    "\n",
    "    axs[0, 1].set_title('acceleration_y [m/s^2]')\n",
    "    axs[0, 1].set_ylim(-1,1)\n",
    "\n",
    "    axs[0, 2].set_title('acceleration_z [m/s^2]')\n",
    "    axs[0, 2].set_ylim(-1,1)\n",
    "\n",
    "    axs[1, 0].set_title('angular_velocity_x [rad/s]')\n",
    "    axs[1, 0].set_ylim(-1,1)\n",
    "\n",
    "    axs[1, 1].set_title('angular_velocity_y [rad/s]')\n",
    "    axs[1, 1].set_ylim(-1,1)\n",
    "\n",
    "    axs[1, 2].set_title('angular_velocity_z [rad/s]')\n",
    "    axs[1, 2].set_ylim(-1,1)\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6AuyxhmIcUa"
   },
   "outputs": [],
   "source": [
    "def plot_all_steps_of_one_trial(fig, axs, malicious = False, data = pd.DataFrame()):\n",
    "  # This function adds the input step data to the existing subplots and plots it\n",
    "  # red if the step was found to be malicious or blue if it was found to be fine.\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    \n",
    "    plt.ioff()\n",
    "\n",
    "    if malicious:\n",
    "        \n",
    "        axs[0, 0].plot(data['acceleration_x [m/s^2]'], c = 'red')\n",
    "        axs[0, 1].plot(data['acceleration_y [m/s^2]'], c = 'red')\n",
    "        axs[0, 2].plot(data['acceleration_z [m/s^2]'], c = 'red')\n",
    "        axs[1, 0].plot(data['angular_velocity_x [rad/s]'], c = 'red')\n",
    "        axs[1, 1].plot(data['angular_velocity_y [rad/s]'], c = 'red')\n",
    "        axs[1, 2].plot(data['angular_velocity_z [rad/s]'], c = 'red')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        axs[0, 0].plot(data['acceleration_x [m/s^2]'], c = 'blue')\n",
    "        axs[0, 1].plot(data['acceleration_y [m/s^2]'], c = 'blue')\n",
    "        axs[0, 2].plot(data['acceleration_z [m/s^2]'], c = 'blue')\n",
    "        axs[1, 0].plot(data['angular_velocity_x [rad/s]'], c = 'blue')\n",
    "        axs[1, 1].plot(data['angular_velocity_y [rad/s]'], c = 'blue')\n",
    "        axs[1, 2].plot(data['angular_velocity_z [rad/s]'], c = 'blue')\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObRAkKkY4Qmr"
   },
   "outputs": [],
   "source": [
    "def remove_malicious_sequences_per_trial(data = pd.DataFrame(), offset_threshold = 0.3, quota_above_threshold_threshold = 0.4,\n",
    "                               difference_std_indivually_threshold = 0.3, difference_std_mean_threshold = 0.2,\n",
    "                               step_length = 100, show_plots = False):\n",
    "  # This funciton takes the data of one trial as the input. First the mean function of all sensors of this trial as well as the average\n",
    "  # standard deviation of the trial are computed. Then all steps of this trial are examinend by checking for 2 things:\n",
    "  # 1.  The data of the step is distant (>'offset_threshold') to the mean data of this trial (sensorwise) for at least x % of the\n",
    "  #     Datapoints of this trial (quota_above_threshold_threshold).\n",
    "  # 2.  The std of this step is different to the mean std of this trial (both as mean of all sensors (difference_std_mean_threshold) and \n",
    "  #     for all sensors seperatly (difference_std_indivually_threshold))\n",
    "  # If one of these thresholds is crossed, the step gets deleted out of the trial data.\n",
    "  # The function returns the data of the trial which only contains the steps which were found to be normal.\n",
    "\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    data = data.drop(columns='index')\n",
    "    \n",
    "    data_out = data\n",
    "    \n",
    "    data_only_sensors = data[['acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]',\n",
    "                                    'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]']]\n",
    "\n",
    "    number_all_steps = int(data_only_sensors.size/(6*step_length))\n",
    "    \n",
    "\n",
    "    subject_number = round(data.iloc[0]['subject_number'])\n",
    "    gait = round(data.iloc[0]['gait'])\n",
    "    trial_number = round(data.iloc[0]['trial_number'])\n",
    "\n",
    "    \n",
    "    for i in range(number_all_steps):\n",
    "        \n",
    "        # getting the information of the current step\n",
    "\n",
    "        current_position = i * step_length\n",
    "\n",
    "        data_step = data.iloc[current_position: current_position+step_length]  \n",
    "\n",
    "        if i == 0:\n",
    "            \n",
    "            # initializing the values for the first step overall\n",
    "            \n",
    "            data_trial_accelerometer_x = pd.DataFrame()\n",
    "            data_trial_accelerometer_y = pd.DataFrame()\n",
    "            data_trial_accelerometer_z = pd.DataFrame()\n",
    "            data_trial_gyroscope_x = pd.DataFrame()\n",
    "            data_trial_gyroscope_y = pd.DataFrame()\n",
    "            data_trial_gyroscope_z = pd.DataFrame()\n",
    "\n",
    "\n",
    "            # now that a new trial is reached the data of the previous trial is examined\n",
    "            # therefore first the mean function of the sensors and the standard deviation are computed\n",
    "\n",
    "\n",
    "            \n",
    "        data_trial_accelerometer_x[str(i)] = np.array(data_step.iloc[:]['acceleration_x [m/s^2]'])\n",
    "        data_trial_accelerometer_y[str(i)] = np.array(data_step.iloc[:]['acceleration_y [m/s^2]'])\n",
    "        data_trial_accelerometer_z[str(i)] = np.array(data_step.iloc[:]['acceleration_z [m/s^2]'])\n",
    "        data_trial_gyroscope_x[str(i)] = np.array(data_step.iloc[:]['angular_velocity_x [rad/s]'])\n",
    "        data_trial_gyroscope_y[str(i)] = np.array(data_step.iloc[:]['angular_velocity_y [rad/s]'])\n",
    "        data_trial_gyroscope_z[str(i)] = np.array(data_step.iloc[:]['angular_velocity_z [rad/s]'])\n",
    "            \n",
    "            \n",
    "    data_trial_mean_accelerometer_x = data_trial_accelerometer_x.mean(axis = 1)\n",
    "    data_trial_mean_accelerometer_y = data_trial_accelerometer_y.mean(axis = 1)\n",
    "    data_trial_mean_accelerometer_z = data_trial_accelerometer_z.mean(axis = 1)\n",
    "    data_trial_mean_gyroscope_x = data_trial_gyroscope_x.mean(axis = 1)\n",
    "    data_trial_mean_gyroscope_y = data_trial_gyroscope_y.mean(axis = 1)\n",
    "    data_trial_mean_gyroscope_z = data_trial_gyroscope_z.mean(axis = 1)\n",
    "\n",
    "\n",
    "    data_trial_means_dict = {'acceleration_x [m/s^2]': data_trial_mean_accelerometer_x, 'acceleration_y [m/s^2]': data_trial_mean_accelerometer_y,\n",
    "                              'acceleration_z [m/s^2]': data_trial_mean_accelerometer_z, 'angular_velocity_x [rad/s]': data_trial_mean_gyroscope_x,\n",
    "                              'angular_velocity_y [rad/s]': data_trial_mean_gyroscope_y, 'angular_velocity_z [rad/s]': data_trial_mean_gyroscope_z}\n",
    "\n",
    "    data_trial_means = pd.DataFrame(data = data_trial_means_dict)\n",
    "\n",
    "\n",
    "    data_trial_std_obj = data_only_sensors.std(axis = 0)\n",
    "    data_trial_std = pd.DataFrame(data_trial_std_obj).T\n",
    "            \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#            \n",
    "    \n",
    "    for i in range(number_all_steps):\n",
    "        \n",
    "        current_position = i * step_length\n",
    "        \n",
    "        data_step = data.iloc[current_position: current_position+step_length]\n",
    "        data_step = data_step.reset_index()\n",
    "        data_step = data_step.drop(columns = 'index')\n",
    "        data_step_only_sensors = data_only_sensors[current_position: current_position + step_length]\n",
    "        data_step_only_sensors = data_step_only_sensors.reset_index()\n",
    "        data_step_only_sensors = data_step_only_sensors.drop(columns = 'index')\n",
    "        \n",
    "        data_step_std_obj = data_step_only_sensors.std(axis = 0)\n",
    "        data_step_std = pd.DataFrame(data_step_std_obj).T\n",
    "    \n",
    "        difference_std = (data_step_std - data_trial_std).abs()\n",
    "        difference_std_mean = float(difference_std.mean(axis = 1))\n",
    "    \n",
    "        axis_list = data_step_only_sensors.columns\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "        if ((difference_std > difference_std_indivually_threshold).any().any()) or (\n",
    "            difference_std_mean > difference_std_mean_threshold):\n",
    "    \n",
    "            # checking if the std exceeds the allowed threshold and if so removing the data of that step\n",
    "    \n",
    "            malicious_std = True\n",
    "        \n",
    "            \n",
    "            step_str = str(round(data_step.iloc[0]['step_number']))\n",
    "    \n",
    "            print_str = 'subject_number' + str(subject_number) + '_gait' + str(gait) + '_trial' + str(trial_number) + '_step' + step_str\n",
    "    \n",
    "            print('The data of %s was found to be malicious and will therefore be removed from the dataset.' % print_str\n",
    "                 + ' The error was a transgression of the standard-deviation-threshold.')  \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                data_out = data_out.drop(list(range(current_position, current_position + step_length)))\n",
    "    \n",
    "            except Exception:\n",
    "                print('There was an error removing the data')                            \n",
    "            \n",
    "    \n",
    "            \n",
    "        else:\n",
    "            malicious_std = False\n",
    "            \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        above_threshold_counter = 0\n",
    "\n",
    "        for j in range(step_length): \n",
    "        \n",
    "        # checking for malicious steps\n",
    "\n",
    "            for axis in axis_list:\n",
    "            \n",
    "            # comparing the data of the current step to the mean of the current trial\n",
    "\n",
    "                data_iteration = float(data_step.iloc[j][axis])\n",
    "\n",
    "                if abs(data_iteration - float(data_trial_means.at[j, axis])) > offset_threshold:\n",
    "                \n",
    "                # checking if the difference between the current value and the mean value exceed the threshold\n",
    "\n",
    "                    above_threshold_counter += 1\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        if (above_threshold_counter/(step_length * 6)) > quota_above_threshold_threshold:\n",
    "                \n",
    "            # checking if the difference between the current value and the mean value exceeded the threshold...\n",
    "            # ... more often than allowed by the quota_above_threshold_threshold (e.g. 40% of the datapoints)...\n",
    "            # ... and if so deleting the data of this step                        \n",
    "            \n",
    "            malicious_offset = True\n",
    "            \n",
    "\n",
    "            step_str = str(round(data_step.iloc[0]['step_number']))\n",
    "\n",
    "            print_str = 'subject_number' + str(subject_number) + '_gait' + str(gait) + '_trial' + str(trial_number) + '_step' + step_str\n",
    "            \n",
    "            print('The data of %s was found to be malicious and will therefore be removed from the dataset.' % print_str\n",
    "                 + ' The error was a transgression of the quota_above_threshold_threshold.')\n",
    "                                            \n",
    "            \n",
    "            try:\n",
    "\n",
    "                data_out = data_out.drop(list(range(current_position, current_position + step_length)))\n",
    "         \n",
    "            except Exception:\n",
    "\n",
    "                print('There was an error removing the data')\n",
    "\n",
    "            \n",
    "        else:\n",
    "            malicious_offset = False\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "  \n",
    "        if show_plots:\n",
    "\n",
    "            if i == 0:\n",
    "\n",
    "\n",
    "                trial_information_str = 'subject_number' + str(subject_number) + '_gait' + str(gait) + '_trial' + str(trial_number)\n",
    "\n",
    "                fig, axs = initialize_plot_all_steps_of_one_trial(trial_information_str)\n",
    "                \n",
    "\n",
    "\n",
    "            if malicious_std or malicious_offset:\n",
    "                malicious = True\n",
    "            else:\n",
    "                malicious = False\n",
    "\n",
    "            fig, axs = plot_all_steps_of_one_trial(fig, axs, malicious, data_step)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    data_out = data_out.reset_index()\n",
    "    data_out = data_out.drop(columns = 'index')\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JRb_z3vhQ1o"
   },
   "source": [
    "## Apply preprocessing steps to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iqBKEw_haqE"
   },
   "outputs": [],
   "source": [
    "# Here all preprocessing steps are applied to the data after loading and preparing (done trialwise)\n",
    "# The data of all trials is added to a long DataFrame which is then output and saved as the data after proprocessing\n",
    "# This data is then later used in the preperation for the NN.\n",
    "\n",
    "datalist = [f for f in listdir(path_to_raw_data) if isfile(join(path_to_raw_data, f))]\n",
    "\n",
    "step_length = 200\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "for filename in datalist:\n",
    "\n",
    "    # Load the raw Data\n",
    "    data = pd.read_csv(path_to_raw_data + filename)\n",
    "    col = data.columns\n",
    "\n",
    "    # Extract motion sequence\n",
    "    data = remove_beginning_and_end(data, standard_deviation_threshold=1)\n",
    "    \n",
    "    if data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Rotate Data\n",
    "    data = rotate_Data(data)   \n",
    "    \n",
    "    # Get starting positions of each step\n",
    "    stepidx = getsteps(data)\n",
    "    \n",
    "    # Filter Data\n",
    "    data = butter_bandpass_DataFrame(data)\n",
    "\n",
    "    # Resample Data of steps to same frame number\n",
    "    data = resample(data = data, stepidx = stepidx, step_length = step_length)\n",
    "    \n",
    "    data = data.drop(columns = ['Time [s]', 'subject_sex', 'subject_age [y]','frequency [Hz]'])\n",
    "    \n",
    "    data = remove_malicious_sequences_per_trial(data, step_length = step_length, offset_threshold=0.4, quota_above_threshold_threshold=0.15, difference_std_indivually_threshold = 0.2, difference_std_mean_threshold = 0.15, show_plots = False)        \n",
    "    \n",
    "    if data.empty:\n",
    "        continue\n",
    "    \n",
    "    if data_all.empty:\n",
    "        data_all = data\n",
    "    else: \n",
    "        data_all = data_all.append(data)\n",
    "\n",
    "        \n",
    "# Saving the data   \n",
    "output_filename = output_directory + '/data_after_preprocessing.csv'\n",
    "\n",
    "data_all.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv_zPxspcxwl"
   },
   "source": [
    "# 3.   Neural Network\n",
    "\n",
    "Input: All preprocesed data as a csv file\n",
    "\n",
    "To include the height or weight in the training, put its prespective boolean to True.\n",
    "\n",
    "The NN-architecture (number of hidden layers, size of each hidden layer) can be set up inside the 'train_NN' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRZi-VswU3Ps"
   },
   "outputs": [],
   "source": [
    "input_file = ''\n",
    "with_height = False\n",
    "with_weight = False\n",
    "number_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frkHgSytU6qq"
   },
   "outputs": [],
   "source": [
    "def prepare_shuffling(data = pd.DataFrame, step_length = 200, weight = False, height = False):\n",
    "  # This function creates a list of arrays where one arrays contains all steps of one subject.\n",
    "  # Depending on the input parameters (weight, height) the input data of the NN is adjusted.\n",
    "  # This list of arrays as well as the amount of columns in the data is then returned from the function.\n",
    " \n",
    "    \n",
    "    if weight and height:\n",
    "        number_columns = 10\n",
    "        data = data[['gait', 'subject_number', 'acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]',\n",
    "                   'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]',\n",
    "                    'subject_height [m]', 'subject_weight [kg]']]\n",
    "        \n",
    "    if weight and not height:\n",
    "        number_columns = 9\n",
    "        data = data[['gait', 'subject_number', 'acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]',\n",
    "                   'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]',\n",
    "                    'subject_weight [kg]']]\n",
    "        \n",
    "    if height and not weight:\n",
    "        number_columns = 9\n",
    "        data = data[['gait', 'subject_number', 'acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]',\n",
    "                   'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]',\n",
    "                    'subject_height [m]']]\n",
    "        \n",
    "    else:\n",
    "        number_columns = 8\n",
    "        data = data[['gait', 'subject_number', 'acceleration_x [m/s^2]', 'acceleration_y [m/s^2]', 'acceleration_z [m/s^2]',\n",
    "                   'angular_velocity_x [rad/s]', 'angular_velocity_y [rad/s]', 'angular_velocity_z [rad/s]']]\n",
    " \n",
    "        \n",
    "    # reshaping the data into a stepwise array (3D)\n",
    "    data_xy = data.to_numpy()\n",
    "    steps = int(np.size(data_xy,0)/step_length)\n",
    "    data_xy = np.array(np.reshape(data_xy,(steps, step_length,number_columns)))\n",
    "    number_of_steps = (steps - (steps%6))\n",
    "    steps_per_part = int(number_of_steps/6)\n",
    "    data_xy = data_xy[:number_of_steps]\n",
    " \n",
    " \n",
    "    # empty arrays\n",
    "    zeros_counting = np.zeros((140))\n",
    " \n",
    "    # finding the amount of steps of each subject and storing them in zeros_counting\n",
    "    for i in range(140):\n",
    "        for j in range(number_of_steps):\n",
    " \n",
    "            if int(round(data_xy[j,0,1])) == i:     \n",
    "                zeros_counting[i] = zeros_counting[i] + 1     \n",
    " \n",
    "    # creating a list object with numpy arrays of the size of each subject (size ~ number of steps of the subject) inside            \n",
    "    zeros_list_subjectwise = []    \n",
    "    for i in range(140):\n",
    "        zeros_list_subjectwise.append(np.zeros((int(zeros_counting[i]),200,number_columns))) \n",
    " \n",
    "    # filling the arrays in the list with the corresponding subject data\n",
    "    for i in range(140):\n",
    "        k=0\n",
    "        for j in range((steps - (steps%6))):\n",
    " \n",
    "            if int(round(data_xy[j,0,1])) == i:\n",
    " \n",
    "                zeros_list_subjectwise[int(round(data_xy[j,0,1]))][k,:,:] = data_xy[j,:,:]\n",
    "                k+=1\n",
    "    \n",
    "    return zeros_list_subjectwise, steps_per_part, number_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqUIB8jzVHi8"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(data, steps_per_part, number_columns):\n",
    "  # This function first shuffles the previously created list of arrays, so that the data is shuffled subjectwise.\n",
    "  # Afterwards the shuffled data is assigned into 6 groups of arrays(5 for the 5fold and 1 as test data). This way all steps\n",
    "  # of all trials of one subject are always in the same group. Finally these 6 groups are all split into NN input (x) and \n",
    "  # desired NN output (y). These arrays are then returned from the funciton and used as the input of the train_NN funciton.\n",
    "\n",
    "    # shuffling the list of arrays\n",
    "    random.seed(45)\n",
    "    random.shuffle(data)\n",
    "    data_subjectwise_shuffled_list = data\n",
    "    \n",
    "    \n",
    "    # converting the list back to a numpy array\n",
    "    list_empty = []\n",
    "    number_empty = 0\n",
    "    for i in range(140):    \n",
    "        if len(data_subjectwise_shuffled_list[i])==0:\n",
    "            number_empty = number_empty + 1\n",
    "            list_empty.append(i) \n",
    "\n",
    "    notempty = 140 - number_empty\n",
    "    length = math.floor(notempty/(number_columns-2))\n",
    "    found = 1\n",
    "    for i in range(140):\n",
    "        if i == 0:\n",
    "            data_train1 = data_subjectwise_shuffled_list[0]\n",
    "            data_train2 = data_subjectwise_shuffled_list[0]\n",
    "            data_train3 = data_subjectwise_shuffled_list[0]\n",
    "            data_train4 = data_subjectwise_shuffled_list[0]\n",
    "            data_train5 = data_subjectwise_shuffled_list[0]\n",
    "            data_test = data_subjectwise_shuffled_list[0]\n",
    "\n",
    "        if i in list_empty:\n",
    "            continue\n",
    "        else:\n",
    "            if found < length + 1:\n",
    "                data_train1 = np.r_[data_train1, data_subjectwise_shuffled_list[i]]\n",
    "            if found > length and found < 2*length + 1:\n",
    "                data_train2 = np.r_[data_train2, data_subjectwise_shuffled_list[i]]\n",
    "            if found > 2*length and found < 3*length + 1:\n",
    "                data_train3 = np.r_[data_train3, data_subjectwise_shuffled_list[i]]\n",
    "            if found > 3*length and found < 4*length + 1:\n",
    "                data_train4 = np.r_[data_train4, data_subjectwise_shuffled_list[i]]\n",
    "            if found > 4*length and found < 5*length + 1:\n",
    "                data_train5 = np.r_[data_train5, data_subjectwise_shuffled_list[i]]\n",
    "            if found > 5*length and found < 6*length + 1:\n",
    "                data_test = np.r_[data_test, data_subjectwise_shuffled_list[i]]   \n",
    "            found = found+1 \n",
    "            \n",
    "    x_train1 = data_train1[:,:,2:]\n",
    "    x_train2 = data_train2[:,:,2:]\n",
    "    x_train3 = data_train3[:,:,2:]\n",
    "    x_train4 = data_train4[:,:,2:]\n",
    "    x_train5 = data_train5[:,:,2:]\n",
    "    x_test = data_test[:,:,2:]\n",
    "\n",
    "    y_train1 = data_train1[:,0,0]\n",
    "    y_train2 = data_train2[:,0,0]\n",
    "    y_train3 = data_train3[:,0,0]\n",
    "    y_train4 = data_train4[:,0,0]\n",
    "    y_train5 = data_train5[:,0,0]\n",
    "    y_test = data_test[:,0,0]\n",
    "    \n",
    "\n",
    "    return x_train1, x_train2, x_train3, x_train4, x_train5, x_test, y_train1, y_train2, y_train3, y_train4, y_train5, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UzpEGhZVKZb"
   },
   "outputs": [],
   "source": [
    "def train_NN(x_train1, x_train2, x_train3, x_train4, x_train5, x_test, y_train1, y_train2, y_train3, y_train4, y_train5, y_test, \n",
    "             epochs = 50, step_length = 200, number_columns = 6):\n",
    "  # This function sets up the 5fold training data into training and validation (each run of the loop). Additionaly some monitoring functions are added\n",
    "  # to stop overfitting and enable early stopping as well as saving the model.\n",
    "  # Afterwards in each loop iteration the model is trained with the set NN-architecture and the respective training and validation data.\n",
    "  # After the 5fold has finished the NN is evaluated with the test data.\n",
    "\n",
    "    filepath='weights.best.hdf5'\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max') # Callback to save the Keras model or model weights \n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1,mode='min') # Early stopping function to prevent overfitting\n",
    "    callback = [checkpointer, earlystopper]\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        print('\\nFold: %s\\n' %(i+1))\n",
    "\n",
    "        if i == 0:\n",
    "            x_train = np.r_[x_train2, x_train3, x_train4, x_train5]\n",
    "            y_train = np.r_[y_train2, y_train3, y_train4, y_train5]\n",
    "            x_val = x_train1\n",
    "            y_val = y_train1\n",
    "        elif i == 1:\n",
    "            x_train = np.r_[x_train1, x_train3, x_train4, x_train5]\n",
    "            y_train = np.r_[y_train1, y_train3, y_train4, y_train5]\n",
    "            x_val = x_train2\n",
    "            y_val = y_train2\n",
    "        elif i == 2:\n",
    "            x_train = np.r_[x_train2, x_train1, x_train4, x_train5]\n",
    "            y_train = np.r_[y_train2, y_train1, y_train4, y_train5]\n",
    "            x_val = x_train3\n",
    "            y_val = y_train3\n",
    "        elif i == 3:\n",
    "            x_train = np.r_[x_train2, x_train3, x_train1, x_train5]\n",
    "            y_train = np.r_[y_train2, y_train3, y_train1, y_train5]\n",
    "            x_val = x_train4\n",
    "            y_val = y_train4\n",
    "        else:\n",
    "            x_train = np.r_[x_train2, x_train3, x_train4, x_train1]\n",
    "            y_train = np.r_[y_train2, y_train3, y_train4, y_train1]\n",
    "            x_val = x_train5\n",
    "            y_val = y_train5\n",
    "\n",
    "\n",
    "# adjust the NN here:            \n",
    "        model=tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(step_length,number_columns-2)), \n",
    "#             tf.keras.layers.Dense(600, activation ='sigmoid', kernel_regularizer = tf.keras.regularizers.l2(0.0001)),\n",
    "#             tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(200, activation ='sigmoid', kernel_regularizer = tf.keras.regularizers.l2(0.0001)), \n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(100, activation ='sigmoid', kernel_regularizer = tf.keras.regularizers.l2(0.0001)), \n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "                                ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer = \"adam\",\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "        # Fit the model\n",
    "        nn = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val), verbose = 1, callbacks = callback)\n",
    "\n",
    "    print('Training done!\\n\\nTesting results:')\n",
    "        \n",
    "    model.evaluate(x_test, y_test, verbose = 1)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7UE0ttLVMu4"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCv8IwheVWYA"
   },
   "outputs": [],
   "source": [
    "data_for_shuffling, steps_per_part, number_columns  = prepare_shuffling(data = data, height = with_height, weight = with_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r99uT5rhVYos"
   },
   "outputs": [],
   "source": [
    "x_train1, x_train2, x_train3, x_train4, x_train5, x_test, y_train1, y_train2, y_train3, y_train4, y_train5, y_test = shuffle_data(data_for_shuffling, steps_per_part, number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz60SdhkVaeu"
   },
   "outputs": [],
   "source": [
    "NN = train_NN(x_train1, x_train2, x_train3, x_train4, x_train5, x_test, y_train1, y_train2, y_train3, y_train4, y_train5, y_test, \n",
    "              epochs = number_epochs, step_length = 200, number_columns = number_columns)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project_A42.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
